# Scanner 3D com SegmentaÃ§Ã£o por IA

## ğŸ“‹ VisÃ£o Geral
Este cÃ³digo implementa um **scanner 3D inteligente** que utiliza InteligÃªncia Artificial para identificar, segmentar e digitalizar objetos especÃ­ficos em uma cena. O sistema combina visÃ£o computacional (YOLOv8) com sensores de profundidade (Intel RealSense) para criar modelos 3D precisos de objetos individuais, ignorando completamente o fundo.

## ğŸ¯ Objetivo Principal
Criar **nuvens de pontos 3D** (point clouds) de objetos especÃ­ficos com recorte perfeito, eliminando automaticamente o fundo e outros elementos indesejados da cena. O sistema:
- âœ… Detecta mÃºltiplos objetos simultaneamente
- âœ… Identifica automaticamente o objeto mais prÃ³ximo
- âœ… Cria mÃ¡scaras precisas pixel a pixel
- âœ… Gera arquivo 3D (.ply) apenas do objeto selecionado

---

## ğŸ”§ Tecnologias Utilizadas

### Hardware
- **Intel RealSense D435/D455**: CÃ¢mera RGB-D (cor + profundidade)
- **Sensores sincronizados**: Captura alinhada de cor e profundidade

### Software
- **YOLOv8-Segmentation**: Modelo de IA para detecÃ§Ã£o e segmentaÃ§Ã£o de objetos
- **Ultralytics**: Framework para modelos YOLO
- **Open3D**: Biblioteca para processamento de nuvens de pontos 3D
- **PyRealSense2**: Interface Python para cÃ¢meras RealSense
- **OpenCV**: Processamento de imagem
- **NumPy**: ManipulaÃ§Ã£o de arrays

---

## ğŸ§  O Diferencial: SegmentaÃ§Ã£o vs DetecÃ§Ã£o

### DetecÃ§Ã£o Tradicional (Bounding Box)
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸ              â”‚  â† Caixa retangular
â”‚     Pessoa      â”‚     Inclui fundo
â”‚                 â”‚     Recorte impreciso
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### SegmentaÃ§Ã£o por IA (Este Sistema)
```
    â•±â•²
   â”‚ğŸ‘¤â”‚  â† MÃ¡scara pixel-perfeita
   â•±  â•²     Apenas o objeto
  â•±â”€â”€â”€â”€â•²    Sem fundo!
```

O modelo **YOLOv8-seg** (sufixo 'seg') gera **mÃ¡scaras binÃ¡rias** que identificam exatamente quais pixels pertencem ao objeto, permitindo um recorte cirÃºrgico.

---

## ğŸ—ï¸ Arquitetura do Sistema

### Fluxo de Dados
```
CÃ¢mera RealSense
    â†“
[Imagem RGB] + [Mapa de Profundidade]
    â†“
YOLOv8-seg (InteligÃªncia Artificial)
    â†“
MÃ¡scara BinÃ¡ria do Objeto
    â†“
AplicaÃ§Ã£o da MÃ¡scara em RGB e Depth
    â†“
Nuvem de Pontos 3D (Point Cloud)
    â†“
Arquivo .ply (Modelo 3D)
```

---

## ğŸ“¦ Componentes Principais

### 1ï¸âƒ£ InicializaÃ§Ã£o do Modelo de IA

```python
model = YOLO("yolov8n-seg.pt")
```

**O que Ã©?**
- Modelo de deep learning prÃ©-treinado
- Reconhece 80 classes de objetos (pessoa, carro, cadeira, etc.)
- Gera mÃ¡scaras de segmentaÃ§Ã£o em tempo real
- VersÃ£o 'n' (nano) = mais rÃ¡pida, ideal para tempo real

**ConfiguraÃ§Ã£o Importante:**
```python
results = model(color_image, stream=True, verbose=False, retina_masks=True)
```
- `stream=True`: Processa frame a frame eficientemente
- `retina_masks=True`: MÃ¡scaras na resoluÃ§Ã£o original (640x480), nÃ£o reduzidas
- `verbose=False`: NÃ£o imprime logs a cada frame

---

### 2ï¸âƒ£ ConfiguraÃ§Ã£o da CÃ¢mera RealSense

```python
config.enable_stream(rs.stream.depth, 640, 480, rs.format.z16, 30)
config.enable_stream(rs.stream.color, 640, 480, rs.format.bgr8, 30)
```

**Streams Capturados:**
- **Depth (z16)**: Profundidade de cada pixel em milÃ­metros (16 bits)
- **Color (bgr8)**: Imagem RGB colorida (formato BGR do OpenCV)
- **30 FPS**: Taxa de atualizaÃ§Ã£o para fluidez

**Alinhamento CrÃ­tico:**
```python
align = rs.align(rs.stream.color)
aligned_frames = align.process(frames)
```
- Alinha o mapa de profundidade com a imagem RGB
- Garante que cada pixel de cor corresponda exatamente ao pixel de profundidade
- Essencial para criar nuvem de pontos colorida precisa

---

## ğŸ­ Processamento da MÃ¡scara (NÃºcleo do Sistema)

### Etapa 1: InferÃªncia da IA
```python
for r in results:
    if r.boxes and r.masks:
        # Modelo detectou objetos com mÃ¡scaras
```

A IA retorna:
- **Boxes**: Caixas delimitadoras (x1, y1, x2, y2)
- **Masks**: MÃ¡scaras de segmentaÃ§Ã£o (formato bitmap)
- **Classes**: Tipo do objeto (pessoa=0, carro=2, cadeira=56, etc.)
- **Confidence**: ConfianÃ§a da detecÃ§Ã£o (0.0 a 1.0)

### Etapa 2: Redimensionamento da MÃ¡scara
```python
mask_raw = r.masks.data[i].cpu().numpy()

if mask_raw.shape[:2] != (480, 640):
    mask_resized = cv2.resize(mask_raw, (640, 480))
```

**Por que redimensionar?**
- A YOLO processa internamente em tamanhos variados para otimizaÃ§Ã£o
- Precisamos que a mÃ¡scara tenha exatamente 640x480 pixels
- Cada pixel da mÃ¡scara deve corresponder a um pixel da imagem

### Etapa 3: BinarizaÃ§Ã£o
```python
binary_mask = (mask_resized > 0.5).astype(np.uint8) * 255
```

**Transforma mÃ¡scara de probabilidade em mÃ¡scara binÃ¡ria:**
- Valores > 0.5 â†’ 255 (branco = objeto)
- Valores â‰¤ 0.5 â†’ 0 (preto = fundo)

### Etapa 4: AplicaÃ§Ã£o Seletiva
```python
masked_depth = depth_image.copy()
masked_depth[binary_mask == 0] = 0  # Zera profundidade do fundo
```

**MÃ¡gica do recorte:**
- Copia o mapa de profundidade completo
- Onde a mÃ¡scara Ã© preta (fundo), zera a profundidade
- Resultado: apenas o objeto tem valores de profundidade vÃ¡lidos

---

## ğŸ“ CÃ¡lculo de DistÃ¢ncia Preciso

### TÃ©cnica: Profundidade Mascarada

```python
masked_depth = depth_image.copy()
masked_depth[binary_mask == 0] = 0

valid_pixels = masked_depth[masked_depth > 0]
dist_meters = np.median(valid_pixels) * depth_scale
```

**Vantagens sobre caixa retangular:**

| MÃ©todo | Problema | Este Sistema |
|--------|----------|--------------|
| Caixa Retangular | Inclui profundidade do fundo | âœ… Ignora fundo completamente |
| MÃ©dia Simples | Afetada por outliers | âœ… Usa mediana (robusto) |
| Ponto Central | Pode estar fora do objeto | âœ… Considera todos os pixels do objeto |

---

## ğŸ¯ LÃ³gica de SeleÃ§Ã£o: "Objeto Mais PrÃ³ximo"

```python
min_dist_detected = float('inf')

for i, box in enumerate(r.boxes):
    dist_meters = np.median(valid_pixels) * depth_scale
    
    if dist_meters < min_dist_detected:
        min_dist_detected = dist_meters
        target_object = {
            "label": label,
            "dist": dist_meters,
            "mask": binary_mask
        }
        color_contour = (0, 255, 0)  # Verde = Alvo selecionado
```

**Funcionamento:**
1. Sistema detecta mÃºltiplos objetos na cena
2. Calcula distÃ¢ncia de cada um usando mÃ¡scara
3. Marca em **verde** o mais prÃ³ximo (alvo do scan)
4. Marca em **amarelo** os demais (apenas informaÃ§Ã£o)
5. Ao pressionar 'S', salva apenas o objeto verde

---

## ğŸ¨ VisualizaÃ§Ã£o em Tempo Real

### Overlay Colorido
```python
contours, _ = cv2.findContours(binary_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
cv2.drawContours(display_image, contours, -1, color_contour, 2)
```

**O que o usuÃ¡rio vÃª:**
- Contorno colorido ao redor de cada objeto detectado
- ğŸŸ¢ **Verde**: Objeto alvo (serÃ¡ salvo ao pressionar 'S')
- ğŸŸ¡ **Amarelo**: Outros objetos detectados
- Texto com classe e distÃ¢ncia em metros

---

## ğŸ—‚ï¸ GeraÃ§Ã£o da Nuvem de Pontos 3D

### Processo "CirÃºrgico" em 7 Etapas

#### Etapa 1: Mascarar a Imagem Colorida
```python
color_rgb = color_image[:, :, ::-1].copy()  # BGR -> RGB
final_color = cv2.bitwise_and(color_rgb, color_rgb, mask=target_object['mask'])
```
- Converte BGR (OpenCV) para RGB (Open3D)
- Aplica mÃ¡scara: pixels do fundo viram pretos
- Resultado: apenas o objeto mantÃ©m cor

#### Etapa 2: Mascarar a Profundidade
```python
final_depth = depth_image.copy()
final_depth[target_object['mask'] == 0] = 0
```
- Copia mapa de profundidade
- Zera profundidade onde mÃ¡scara Ã© preta
- Resultado: apenas o objeto tem valores de distÃ¢ncia

#### Etapa 3: Converter para Open3D
```python
o3d_color = o3d.geometry.Image(final_color)
o3d_depth = o3d.geometry.Image(final_depth)
```
- Converte arrays NumPy para formato Open3D
- Prepara para criar geometria 3D

#### Etapa 4: Obter ParÃ¢metros IntrÃ­nsecos
```python
intrinsics = color_frame.profile.as_video_stream_profile().intrinsics
o3d_intrinsics = o3d.camera.PinholeCameraIntrinsic(
    intrinsics.width, intrinsics.height,
    intrinsics.fx, intrinsics.fy,
    intrinsics.ppx, intrinsics.ppy
)
```

**O que sÃ£o intrÃ­nsecos?**
- **fx, fy**: DistÃ¢ncia focal (zoom) em pixels
- **ppx, ppy**: Centro Ã³ptico (onde o eixo da lente cruza o sensor)
- NecessÃ¡rios para converter pixels 2D â†’ pontos 3D
- Cada cÃ¢mera tem valores Ãºnicos (calibraÃ§Ã£o de fÃ¡brica)

#### Etapa 5: Criar Imagem RGBD
```python
rgbd_image = o3d.geometry.RGBDImage.create_from_color_and_depth(
    o3d_color,
    o3d_depth,
    depth_scale=1.0 / depth_scale,
    depth_trunc=10.0,
    convert_rgb_to_intensity=False
)
```

**ParÃ¢metros importantes:**
- `depth_scale`: Converte valores brutos para metros
- `depth_trunc=10.0`: Ignora pontos alÃ©m de 10 metros (ruÃ­do)
- `convert_rgb_to_intensity=False`: MantÃ©m cores RGB originais

#### Etapa 6: Gerar Point Cloud
```python
pcd = o3d.geometry.PointCloud.create_from_rgbd_image(
    rgbd_image,
    o3d_intrinsics
)
```

**O que acontece internamente:**
- Para cada pixel nÃ£o-zero na profundidade:
  - Calcula posiÃ§Ã£o (X, Y, Z) no espaÃ§o 3D
  - Atribui cor RGB do pixel correspondente
- Resultado: milhares de pontos coloridos no espaÃ§o 3D

#### Etapa 7: CorreÃ§Ã£o de OrientaÃ§Ã£o
```python
pcd.transform([[1, 0, 0, 0], [0, -1, 0, 0], [0, 0, -1, 0], [0, 0, 0, 1]])
```

**Por que transformar?**
- RealSense usa coordenadas: X=direita, Y=baixo, Z=frente
- ConvenÃ§Ã£o 3D padrÃ£o: X=direita, Y=cima, Z=fundo
- Matriz inverte Y e Z para alinhamento correto

#### Etapa 8: Salvar Arquivo
```python
timestamp = datetime.datetime.now().strftime("%H%M%S")
filename = f"recorte_{target_object['label']}_{timestamp}.ply"
o3d.io.write_point_cloud(filename, pcd)
```

**Formato PLY:**
- Formato aberto para nuvens de pontos
- Pode ser aberto em: MeshLab, CloudCompare, Blender
- ContÃ©m posiÃ§Ãµes (x,y,z) e cores (r,g,b) de cada ponto

---

## ğŸ® Controles do Sistema

| Tecla | AÃ§Ã£o | DescriÃ§Ã£o |
|-------|------|-----------|
| **S** | Salvar | Captura e salva nuvem de pontos 3D do objeto verde |
| **Q** ou **ESC** | Sair | Encerra o scanner |

---

## ğŸ“Š Exemplo de Uso PrÃ¡tico

### CenÃ¡rio: Digitalizar uma Cadeira

1. **Sistema Iniciado**
   ```
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ ğŸª‘ Cadeira (1.2m)   â”‚ â† Verde (alvo)
   â”‚ ğŸ‘¤ Pessoa (2.5m)    â”‚ â† Amarelo
   â”‚ ğŸšª Porta (3.0m)     â”‚ â† Amarelo
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   ```

2. **UsuÃ¡rio pressiona 'S'**
   - Sistema processa apenas a cadeira (objeto verde)
   - Aplica mÃ¡scara de segmentaÃ§Ã£o
   - Gera nuvem de pontos 3D

3. **Arquivo Salvo**
   ```
   recorte_chair_143052.ply
   ```
   - ContÃ©m apenas a cadeira
   - Pessoa e porta nÃ£o estÃ£o no arquivo
   - Fundo completamente removido

4. **VisualizaÃ§Ã£o no MeshLab**
   ```
        â•±â•²
       â•±  â•²
      â”‚ ğŸª‘ â”‚  â† Modelo 3D limpo
      â•±â”€â”€â”€â”€â•²    Sem fundo!
     â•±      â•²
   ```

---

## ğŸ“ Conceitos para Explicar ao Orientando

### 1. O que Ã© SegmentaÃ§Ã£o SemÃ¢ntica?
- **ClassificaÃ§Ã£o pixel a pixel** de uma imagem
- Cada pixel recebe um rÃ³tulo (pessoa, carro, fundo, etc.)
- Diferente de detecÃ§Ã£o (caixas) ou classificaÃ§Ã£o (imagem inteira)

### 2. Por que YOLOv8 e nÃ£o outro modelo?
- âš¡ **Velocidade**: Processa 30+ FPS em tempo real
- ğŸ¯ **PrecisÃ£o**: Estado da arte em segmentaÃ§Ã£o
- ğŸ“¦ **Facilidade**: Uma linha de cÃ³digo para inferÃªncia
- ğŸŒ **Versatilidade**: 80 classes prÃ©-treinadas

### 3. O que Ã© uma Nuvem de Pontos?
- Conjunto de pontos (x, y, z) no espaÃ§o 3D
- Cada ponto pode ter cor (r, g, b)
- Representa a superfÃ­cie de um objeto
- Pode ser convertida em malha (mesh) depois

### 4. Por que Alinhar Depth com Color?
- Sensores RGB e Depth sÃ£o fÃ­sicos diferentes
- EstÃ£o em posiÃ§Ãµes ligeiramente diferentes na cÃ¢mera
- Alinhamento garante: pixel[100,100] na cor = pixel[100,100] na profundidade
- Sem alinhamento: cores aparecem deslocadas no modelo 3D

### 5. IntrÃ­nsecos vs ExtrÃ­nsecos
- **IntrÃ­nsecos**: Propriedades internas da cÃ¢mera (focal, centro Ã³ptico)
- **ExtrÃ­nsecos**: PosiÃ§Ã£o/rotaÃ§Ã£o da cÃ¢mera no mundo
- Este cÃ³digo usa apenas intrÃ­nsecos (cÃ¢mera Ã© referÃªncia)

---

## ğŸ”„ Fluxo Completo (Diagrama Detalhado)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. CAPTURA                                              â”‚
â”‚    RealSense â†’ [RGB 640x480] + [Depth 640x480]         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. ALINHAMENTO                                          â”‚
â”‚    Sincroniza RGB com Depth pixel a pixel               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. SEGMENTAÃ‡ÃƒO IA                                       â”‚
â”‚    YOLOv8 â†’ Detecta objetos + Gera mÃ¡scaras binÃ¡rias    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. SELEÃ‡ÃƒO                                              â”‚
â”‚    Calcula distÃ¢ncia de cada objeto â†’ Escolhe o +prÃ³ximoâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 5. VISUALIZAÃ‡ÃƒO                                         â”‚
â”‚    Desenha contornos coloridos + Aguarda comando        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â†“ (UsuÃ¡rio pressiona 'S')
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 6. APLICAÃ‡ÃƒO DE MÃSCARA                                 â”‚
â”‚    RGB mascarado + Depth mascarado                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 7. GERAÃ‡ÃƒO 3D                                           â”‚
â”‚    IntrÃ­nsecos + RGBD â†’ Point Cloud                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 8. EXPORTAÃ‡ÃƒO                                           â”‚
â”‚    Arquivo .ply salvo no disco                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## âš ï¸ LimitaÃ§Ãµes e ConsideraÃ§Ãµes

### LimitaÃ§Ãµes Atuais
1. **Classes Fixas**: Reconhece apenas 80 classes do COCO dataset
2. **DistÃ¢ncia Limitada**: Depth funciona melhor atÃ© 5 metros
3. **SuperfÃ­cies Reflexivas**: Vidro/espelhos causam ruÃ­do na profundidade
4. **Objetos Pequenos**: Objetos < 5cm podem nÃ£o ser detectados
5. **Um Objeto por Vez**: Salva apenas o objeto mais prÃ³ximo

### Boas PrÃ¡ticas
- âœ… IluminaÃ§Ã£o adequada melhora detecÃ§Ã£o da IA
- âœ… Mantenha objetos entre 0.5m e 4m da cÃ¢mera
- âœ… Evite movimentos rÃ¡pidos durante captura
- âœ… Capture mÃºltiplos Ã¢ngulos para reconstruÃ§Ã£o completa

---

## ğŸš€ AplicaÃ§Ãµes PrÃ¡ticas

### 1. E-commerce
- DigitalizaÃ§Ã£o automÃ¡tica de produtos
- CriaÃ§Ã£o de catÃ¡logos 3D
- VisualizaÃ§Ã£o AR para clientes

### 2. Manufatura
- InspeÃ§Ã£o de qualidade dimensional
- ComparaÃ§Ã£o com modelos CAD
- DocumentaÃ§Ã£o de peÃ§as

### 3. Arqueologia
- DigitalizaÃ§Ã£o de artefatos
- PreservaÃ§Ã£o digital de patrimÃ´nio
- AnÃ¡lise nÃ£o-invasiva

### 4. EducaÃ§Ã£o
- CriaÃ§Ã£o de bibliotecas 3D
- Material didÃ¡tico interativo
- Ensino de modelagem 3D

---

## ğŸ› ï¸ PossÃ­veis Melhorias

### NÃ­vel Iniciante
- [ ] Adicionar contador de objetos detectados
- [ ] Salvar tambÃ©m imagem 2D do objeto
- [ ] Adicionar filtro por classe (sÃ³ cadeiras, sÃ³ pessoas)

### NÃ­vel IntermediÃ¡rio
- [ ] Captura multi-Ã¢ngulo automÃ¡tica (360Â°)
- [ ] FusÃ£o de mÃºltiplas capturas em um modelo Ãºnico
- [ ] Interface grÃ¡fica (GUI) para seleÃ§Ã£o de objetos

### NÃ­vel AvanÃ§ado
- [ ] Treinar modelo customizado para classes especÃ­ficas
- [ ] ReconstruÃ§Ã£o de malha (mesh) completa
- [ ] TexturizaÃ§Ã£o UV para realismo
- [ ] IntegraÃ§Ã£o com CAD para mediÃ§Ãµes precisas

---

## ğŸ”— DependÃªncias e InstalaÃ§Ã£o

### PrÃ©-requisitos
```bash
pip install pyrealsense2
pip install open3d
pip install opencv-python
pip install ultralytics
pip install numpy
```

### Modelo YOLO
```bash
# O modelo serÃ¡ baixado automaticamente na primeira execuÃ§Ã£o
# Ou baixe manualmente de: https://github.com/ultralytics/assets/releases
```

### ExecuÃ§Ã£o
```bash
python virtualizacao_v2.py
```

---

## ğŸ¤– IntegraÃ§Ã£o com SAM (Segment Anything Model) da Meta

### O que Ã© o SAM?

**SAM (Segment Anything Model)** Ã© um modelo revolucionÃ¡rio de segmentaÃ§Ã£o desenvolvido pela Meta AI que pode segmentar **qualquer objeto** em uma imagem, sem necessidade de treinamento prÃ©vio para classes especÃ­ficas.

### SAM vs YOLOv8: ComparaÃ§Ã£o

| CaracterÃ­stica | YOLOv8-seg | SAM (Meta) |
|----------------|------------|------------|
| **Classes** | 80 classes prÃ©-definidas | Qualquer objeto |
| **Treinamento** | NecessÃ¡rio para novas classes | Zero-shot (funciona direto) |
| **Prompt** | AutomÃ¡tico | Requer interaÃ§Ã£o (clique/caixa) |
| **Velocidade** | Muito rÃ¡pido (30+ FPS) | Mais lento (~5 FPS) |
| **PrecisÃ£o** | Boa para classes conhecidas | Excelente para qualquer objeto |
| **Uso Ideal** | DetecÃ§Ã£o automÃ¡tica em tempo real | SegmentaÃ§Ã£o interativa precisa |

### Quando Usar SAM no Sistema?

âœ… **Use SAM quando:**
- Precisa segmentar objetos Ãºnicos/incomuns (nÃ£o nas 80 classes do YOLO)
- Quer controle manual sobre o que segmentar (clique do usuÃ¡rio)
- PrecisÃ£o Ã© mais importante que velocidade
- Trabalha com objetos complexos ou parcialmente ocultos

âŒ **Mantenha YOLOv8 quando:**
- Precisa de detecÃ§Ã£o automÃ¡tica sem interaÃ§Ã£o humana
- Velocidade em tempo real Ã© crÃ­tica
- Objetos sÃ£o de classes comuns (pessoa, carro, cadeira, etc.)
- Quer processar mÃºltiplos objetos simultaneamente

---

## ğŸ”§ Como Integrar SAM ao Sistema Atual

### Passo 1: InstalaÃ§Ã£o

```bash
# Instalar SAM
pip install segment-anything

# Baixar modelo (escolha um):
# - sam_vit_h (Huge): Mais preciso, mais lento
# - sam_vit_l (Large): Balanceado
# - sam_vit_b (Base): Mais rÃ¡pido, menos preciso

# Baixar de: https://github.com/facebookresearch/segment-anything
```

### Passo 2: CÃ³digo de IntegraÃ§Ã£o

Aqui estÃ¡ a versÃ£o **hÃ­brida** que combina YOLOv8 (detecÃ§Ã£o automÃ¡tica) + SAM (refinamento preciso):

```python
import pyrealsense2 as rs
import numpy as np
import open3d as o3d
import cv2
import datetime
from ultralytics import YOLO
from segment_anything import sam_model_registry, SamPredictor


def scan_with_sam():
    """
    Sistema HÃ­brido:
    1. YOLO detecta objetos automaticamente
    2. UsuÃ¡rio clica para refinar com SAM
    3. SAM gera mÃ¡scara perfeita
    """
    
    # --- INICIALIZAÃ‡ÃƒO ---
    print("Carregando YOLOv8 (detecÃ§Ã£o rÃ¡pida)...")
    yolo_model = YOLO("yolov8n-seg.pt")
    
    print("Carregando SAM (segmentaÃ§Ã£o precisa)...")
    sam_checkpoint = "sam_vit_b_01ec64.pth"  # Ajuste o caminho
    model_type = "vit_b"
    device = "cuda"  # ou "cpu" se nÃ£o tiver GPU
    
    sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)
    sam.to(device=device)
    sam_predictor = SamPredictor(sam)
    
    # ConfiguraÃ§Ã£o RealSense (igual ao cÃ³digo original)
    pipeline = rs.pipeline()
    config = rs.config()
    config.enable_stream(rs.stream.depth, 640, 480, rs.format.z16, 30)
    config.enable_stream(rs.stream.color, 640, 480, rs.format.bgr8, 30)
    
    profile = pipeline.start(config)
    depth_sensor = profile.get_device().first_depth_sensor()
    depth_scale = depth_sensor.get_depth_scale()
    
    align = rs.align(rs.stream.color)
    
    # VariÃ¡veis de controle
    modo_sam = False  # True = SAM ativo, False = YOLO ativo
    pontos_clicados = []
    cor_image_atual = None
    
    def mouse_callback(event, x, y, flags, param):
        """Captura cliques do mouse para SAM"""
        nonlocal pontos_clicados, modo_sam
        
        if modo_sam and event == cv2.EVENT_LBUTTONDOWN:
            pontos_clicados.append([x, y])
            print(f"Ponto adicionado: ({x}, {y})")
    
    cv2.namedWindow('Scanner Hibrido')
    cv2.setMouseCallback('Scanner Hibrido', mouse_callback)
    
    try:
        while True:
            frames = pipeline.wait_for_frames()
            aligned_frames = align.process(frames)
            aligned_depth_frame = aligned_frames.get_depth_frame()
            color_frame = aligned_frames.get_color_frame()
            
            if not aligned_depth_frame or not color_frame:
                continue
            
            depth_image = np.asanyarray(aligned_depth_frame.get_data())
            color_image = np.asanyarray(color_frame.get_data())
            cor_image_atual = color_image.copy()
            
            display_image = color_image.copy()
            
            # --- MODO 1: YOLO (DetecÃ§Ã£o AutomÃ¡tica) ---
            if not modo_sam:
                results = yolo_model(color_image, stream=True, verbose=False, retina_masks=True)
                
                for r in results:
                    if r.boxes and r.masks:
                        for i, box in enumerate(r.boxes):
                            x1, y1, x2, y2 = map(int, box.xyxy[0])
                            label = yolo_model.names[int(box.cls[0])]
                            conf = float(box.conf[0])
                            
                            if conf < 0.5: continue
                            
                            # Desenhar detecÃ§Ãµes
                            cv2.rectangle(display_image, (x1, y1), (x2, y2), (0, 255, 0), 2)
                            cv2.putText(display_image, f"{label} {conf:.2f}", 
                                       (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,255,0), 2)
                
                cv2.putText(display_image, "MODO YOLO - Pressione 'M' para SAM", 
                           (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0,255,0), 2)
            
            # --- MODO 2: SAM (SegmentaÃ§Ã£o Interativa) ---
            else:
                # Preparar imagem para SAM (RGB)
                sam_predictor.set_image(cv2.cvtColor(cor_image_atual, cv2.COLOR_BGR2RGB))
                
                # Se houver pontos clicados, gerar mÃ¡scara
                if len(pontos_clicados) > 0:
                    input_points = np.array(pontos_clicados)
                    input_labels = np.ones(len(pontos_clicados))  # 1 = foreground
                    
                    # Gerar mÃ¡scara com SAM
                    masks, scores, logits = sam_predictor.predict(
                        point_coords=input_points,
                        point_labels=input_labels,
                        multimask_output=True  # Gera 3 opÃ§Ãµes
                    )
                    
                    # Pegar a melhor mÃ¡scara (maior score)
                    best_mask = masks[np.argmax(scores)]
                    
                    # Overlay colorido da mÃ¡scara
                    overlay = display_image.copy()
                    overlay[best_mask] = overlay[best_mask] * 0.5 + np.array([0, 255, 0]) * 0.5
                    display_image = overlay.astype(np.uint8)
                    
                    # Desenhar contorno
                    mask_uint8 = (best_mask * 255).astype(np.uint8)
                    contours, _ = cv2.findContours(mask_uint8, cv2.RETR_EXTERNAL, 
                                                   cv2.CHAIN_APPROX_SIMPLE)
                    cv2.drawContours(display_image, contours, -1, (0, 255, 255), 2)
                
                # Desenhar pontos clicados
                for pt in pontos_clicados:
                    cv2.circle(display_image, tuple(pt), 5, (255, 0, 0), -1)
                
                cv2.putText(display_image, f"MODO SAM - Clique no objeto ({len(pontos_clicados)} pontos)", 
                           (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0,255,255), 2)
                cv2.putText(display_image, "'S'=Salvar | 'C'=Limpar | 'M'=YOLO", 
                           (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255,255,255), 2)
            
            cv2.imshow('Scanner Hibrido', display_image)
            
            # --- CONTROLES ---
            key = cv2.waitKey(1)
            
            if key & 0xFF == ord('q') or key == 27:
                break
            
            elif key & 0xFF == ord('m'):
                # Alternar entre YOLO e SAM
                modo_sam = not modo_sam
                pontos_clicados = []
                print(f"Modo alterado para: {'SAM' if modo_sam else 'YOLO'}")
            
            elif key & 0xFF == ord('c'):
                # Limpar pontos
                pontos_clicados = []
                print("Pontos limpos")
            
            elif key & 0xFF == ord('s'):
                if modo_sam and len(pontos_clicados) > 0:
                    print("Salvando com mÃ¡scara SAM...")
                    
                    # Gerar mÃ¡scara final
                    sam_predictor.set_image(cv2.cvtColor(cor_image_atual, cv2.COLOR_BGR2RGB))
                    input_points = np.array(pontos_clicados)
                    input_labels = np.ones(len(pontos_clicados))
                    
                    masks, scores, _ = sam_predictor.predict(
                        point_coords=input_points,
                        point_labels=input_labels,
                        multimask_output=True
                    )
                    
                    best_mask = masks[np.argmax(scores)]
                    binary_mask = (best_mask * 255).astype(np.uint8)
                    
                    # Aplicar mÃ¡scara (igual ao cÃ³digo original)
                    color_rgb = cor_image_atual[:, :, ::-1].copy()
                    final_color = cv2.bitwise_and(color_rgb, color_rgb, mask=binary_mask)
                    
                    final_depth = depth_image.copy()
                    final_depth[binary_mask == 0] = 0
                    
                    # Criar Point Cloud
                    o3d_color = o3d.geometry.Image(final_color)
                    o3d_depth = o3d.geometry.Image(final_depth)
                    
                    intrinsics = color_frame.profile.as_video_stream_profile().intrinsics
                    o3d_intrinsics = o3d.camera.PinholeCameraIntrinsic(
                        intrinsics.width, intrinsics.height,
                        intrinsics.fx, intrinsics.fy,
                        intrinsics.ppx, intrinsics.ppy
                    )
                    
                    rgbd_image = o3d.geometry.RGBDImage.create_from_color_and_depth(
                        o3d_color, o3d_depth,
                        depth_scale=1.0/depth_scale,
                        depth_trunc=10.0,
                        convert_rgb_to_intensity=False
                    )
                    
                    pcd = o3d.geometry.PointCloud.create_from_rgbd_image(
                        rgbd_image, o3d_intrinsics
                    )
                    
                    pcd.transform([[1,0,0,0], [0,-1,0,0], [0,0,-1,0], [0,0,0,1]])
                    
                    timestamp = datetime.datetime.now().strftime("%H%M%S")
                    filename = f"sam_scan_{timestamp}.ply"
                    o3d.io.write_point_cloud(filename, pcd)
                    
                    print(f"âœ“ Salvo: {filename}")
                    pontos_clicados = []
                    
                else:
                    print("Use modo SAM e clique no objeto primeiro!")
    
    finally:
        pipeline.stop()
        cv2.destroyAllWindows()


if __name__ == "__main__":
    scan_with_sam()
```

### Passo 3: Modos de Uso

#### Modo 1: SAM com Prompts de Ponto (Clique)
```python
# UsuÃ¡rio clica em pontos do objeto
# SAM segmenta automaticamente
# Ideal para: Objetos claros com bordas definidas
```

#### Modo 2: SAM com Bounding Box
```python
# ModificaÃ§Ã£o do cÃ³digo:
def mouse_callback_box(event, x, y, flags, param):
    global bbox_start, bbox_end, drawing
    
    if event == cv2.EVENT_LBUTTONDOWN:
        drawing = True
        bbox_start = (x, y)
    
    elif event == cv2.EVENT_MOUSEMOVE and drawing:
        bbox_end = (x, y)
    
    elif event == cv2.EVENT_LBUTTONUP:
        drawing = False
        bbox_end = (x, y)
        
        # Usar caixa como prompt para SAM
        input_box = np.array([bbox_start[0], bbox_start[1], 
                             bbox_end[0], bbox_end[1]])
        
        masks, scores, _ = sam_predictor.predict(
            box=input_box,
            multimask_output=False
        )
```

#### Modo 3: SAM AutomÃ¡tico (com YOLO)
```python
# Use detecÃ§Ãµes do YOLO como prompts para SAM
# Melhor dos dois mundos:
# - YOLO detecta automaticamente
# - SAM refina a mÃ¡scara com precisÃ£o

for box in yolo_detections:
    x1, y1, x2, y2 = box
    input_box = np.array([x1, y1, x2, y2])
    
    masks, _, _ = sam_predictor.predict(
        box=input_box,
        multimask_output=False
    )
    # Usa mÃ¡scara do SAM em vez da mÃ¡scara YOLO
```

---

## ğŸ¯ EstratÃ©gias de IntegraÃ§Ã£o

### EstratÃ©gia 1: YOLO + SAM (Recomendado)
```
YOLO (rÃ¡pido) â†’ Detecta objetos
    â†“
SAM (preciso) â†’ Refina mÃ¡scaras
    â†“
Point Cloud 3D
```

**Vantagens:**
- AutomÃ¡tico + Preciso
- Sem necessidade de interaÃ§Ã£o do usuÃ¡rio
- Funciona para classes conhecidas e desconhecidas

### EstratÃ©gia 2: SAM Puro (Interativo)
```
UsuÃ¡rio clica â†’ SAM segmenta â†’ Point Cloud 3D
```

**Vantagens:**
- Funciona com qualquer objeto
- Controle total do usuÃ¡rio
- Melhor para objetos Ãºnicos/artÃ­sticos

### EstratÃ©gia 3: SeleÃ§Ã£o Adaptativa
```python
if objeto in classes_yolo:
    usar_yolo()  # RÃ¡pido
else:
    usar_sam()   # Preciso mas mais lento
```

---

## âš¡ OtimizaÃ§Ãµes de Performance

### 1. Cache do Encoder SAM
```python
# Processar imagem uma vez, usar mÃºltiplos prompts
sam_predictor.set_image(image)  # Computacionalmente caro

# Agora pode fazer mÃºltiplas prediÃ§Ãµes rapidamente
mask1 = sam_predictor.predict(point1)
mask2 = sam_predictor.predict(point2)
mask3 = sam_predictor.predict(box1)
```

### 2. Usar SAM Mobile (FastSAM)
```bash
pip install ultralytics  # JÃ¡ tem FastSAM integrado

# No cÃ³digo:
from ultralytics import FastSAM

model = FastSAM('FastSAM-x.pt')
results = model(image, device='cuda', retina_masks=True)
```

**FastSAM vs SAM:**
- 10x mais rÃ¡pido
- Baseado em YOLOv8
- Qualidade 95% do SAM original

### 3. ReduÃ§Ã£o de ResoluÃ§Ã£o
```python
# Processar em resoluÃ§Ã£o menor, depois redimensionar mÃ¡scara
image_small = cv2.resize(image, (320, 240))
sam_predictor.set_image(image_small)
# ... gerar mÃ¡scara
mask_full = cv2.resize(mask, (640, 480))
```

---

## ğŸ“Š Exemplo PrÃ¡tico: Segmentar Objeto Irregular

### CenÃ¡rio: Digitalizar uma escultura complexa

**Problema com YOLO:**
```
âŒ Classe "escultura" nÃ£o existe no COCO dataset
âŒ Formato irregular confunde detecÃ§Ã£o de caixas
âŒ MÃ¡scara imprecisa em detalhes finos
```

**SoluÃ§Ã£o com SAM:**
```
âœ… UsuÃ¡rio clica em 3-4 pontos na escultura
âœ… SAM gera mÃ¡scara perfeita (bordas suaves)
âœ… Point Cloud captura todos os detalhes
âœ… Resultado: Modelo 3D preciso
```

### CÃ³digo EspecÃ­fico:
```python
# UsuÃ¡rio clica em pontos da escultura
pontos = [[320, 240], [350, 200], [300, 280], [340, 260]]
labels = [1, 1, 1, 1]  # Todos foreground

masks, scores, _ = sam_predictor.predict(
    point_coords=np.array(pontos),
    point_labels=np.array(labels),
    multimask_output=True
)

# Escolher mÃ¡scara com maior score
melhor_mascara = masks[np.argmax(scores)]
```

---

## ğŸ”„ Fluxo Completo com SAM

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Captura: RealSense RGB + Depth          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ DetecÃ§Ã£o Inicial (Opcional):           â”‚
â”‚ YOLO identifica Ã¡rea de interesse       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ InteraÃ§Ã£o UsuÃ¡rio:                      â”‚
â”‚ - Cliques em pontos do objeto           â”‚
â”‚ - OU desenho de caixa                   â”‚
â”‚ - OU automÃ¡tico via YOLO box            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ SegmentaÃ§Ã£o SAM:                        â”‚
â”‚ - Encoder processa imagem               â”‚
â”‚ - Decoder gera mÃ¡scara do prompt        â”‚
â”‚ - Retorna 3 opÃ§Ãµes (melhor score)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Refinamento (Opcional):                 â”‚
â”‚ - UsuÃ¡rio adiciona mais pontos          â”‚
â”‚ - SAM regenera mÃ¡scara                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ AplicaÃ§Ã£o de MÃ¡scara:                   â”‚
â”‚ - RGB mascarado                         â”‚
â”‚ - Depth mascarado                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ GeraÃ§Ã£o 3D: Point Cloud â†’ .ply          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“š ReferÃªncias e Recursos

### SAM (Meta AI)
- **Paper**: "Segment Anything" (Kirillov et al., 2023)
- **GitHub**: https://github.com/facebookresearch/segment-anything
- **Demo Online**: https://segment-anything.com/
- **Modelos**: https://github.com/facebookresearch/segment-anything#model-checkpoints

### DocumentaÃ§Ã£o
- **YOLOv8**: https://docs.ultralytics.com/
- **Open3D**: http://www.open3d.org/docs/
- **RealSense**: https://dev.intelrealsense.com/docs/

### Papers Relevantes
- SAM: Kirillov et al. (2023)
- YOLOv8: Ultralytics (2023)
- Open3D: Zhou et al. (2018)
- COCO Dataset: Lin et al. (2014)

### Ferramentas de VisualizaÃ§Ã£o
- **MeshLab**: Viewer gratuito de nuvens de pontos
- **CloudCompare**: AnÃ¡lise avanÃ§ada de point clouds
- **Blender**: EdiÃ§Ã£o e renderizaÃ§Ã£o 3D

---

## ğŸ’¡ Resumo em 3 Pontos

1. **SegmentaÃ§Ã£o por IA**: YOLOv8 identifica objetos e cria mÃ¡scaras perfeitas
2. **Recorte CirÃºrgico**: MÃ¡scara remove fundo de RGB e Depth simultaneamente
3. **Modelo 3D Limpo**: Open3D gera arquivo .ply apenas do objeto desejado

---

**Ãšltima atualizaÃ§Ã£o:** Janeiro 2026  
**Tecnologia:** YOLOv8 + RealSense + Open3D  
**Autor:** Sistema de DigitalizaÃ§Ã£o 3D com IA
